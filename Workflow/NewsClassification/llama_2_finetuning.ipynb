{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MengOonLee/AccountReceivable/blob/main/Workflow/NewsClassification/llama_2_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ced97d8b-f12b-45d1-9664-68f53591d03b",
      "metadata": {
        "id": "ced97d8b-f12b-45d1-9664-68f53591d03b"
      },
      "source": [
        "# Fine-Tuning LLaMA 2 Models with a single GPU and OVHcloud\n",
        "\n",
        "**In this OVHcloud tutorial, we will walk you through the process of fine-tuning LLaMA 2 models, providing step-by-step instructions.**\n",
        "\n",
        "### Introduction\n",
        "\n",
        "On July 18, 2023, [Meta](https://about.meta.com/) released [LLaMA 2](https://ai.meta.com/llama/), the latest version of their Large Language Model (LLM).\n",
        "\n",
        "Trained between January 2023 and July 2023 on 2 trillion tokens, these new models outperforms other LLMs on many benchmarks, including reasoning, coding, proficiency, and knowledge tests. This release comes in different flavors, with parameter sizes of 7B, 13B, and a mind-blowing 70B. Models are intended for free for both commercial and research use in English.\n",
        "\n",
        "To suit every text generation needed and fine-tune these models, we will use [QLoRA (Efficient Finetuning of Quantized LLMs)](https://arxiv.org/abs/2305.14314), a highly efficient fine-tuning technique that involves quantizing a pretrained LLM to just 4 bits and adding small \"Low-Rank Adapters\". This unique approach allows for fine-tuning LLMs using just a single GPU! This technique is supported by the [PEFT library](https://huggingface.co/docs/peft/).\n",
        "\n",
        "### Requirements\n",
        "\n",
        "To successfully fine-tune LLaMA 2 models, you will need the following:\n",
        "\n",
        "- **Set up your Python environment** by installing the `requirements.txt` file\n",
        "- **Llama 2 Model**. To obtain the Llama 2 model, you will need to:\n",
        "    - Fill Meta's form to [request access to the next version of Llama](https://ai.meta.com/resources/models-and-libraries/llama-downloads/). Indeed, the use of Llama 2 is governed by the Meta license, that you must accept in order to download the model weights and tokenizer.\n",
        "    - Have a [Hugging Face](https://huggingface.co/) account (with the same email address you entered in Meta's form).\n",
        "    - Have a [Hugging Face token](https://huggingface.co/settings/tokens).\n",
        "    - Visit the page of one of the LLaMA 2 available models (version [7B](https://huggingface.co/meta-llama/Llama-2-7b-hf), [13B](https://huggingface.co/meta-llama/Llama-2-13b-hf) or [70B](https://huggingface.co/meta-llama/Llama-2-70b-hf)), and accept Hugging Face's license terms and acceptable use policy.\n",
        "    > Once you have accepted this, you will get the following message: *Your request to access this repo has been successfully submitted, and is pending a review from the repo's authors*, which a few hours later should change to: *You have been granted access to this model*.\n",
        "    - Log in to the Hugging Face model Hub from your notebook's terminal. To do this, just click the `+` button and open a terminal. You can also perform this by clicking `File` > `New` > `Terminal`. Then, use the `huggingface-cli login` command, and enter your token. You will not need to add your token as git credential.\n",
        "<br><br>\n",
        "- **Powerful Computing Resources**: Fine-tuning the Llama 2 model requires substantial computational power. Ensure you are running code on GPU(s)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41e88248-466c-476e-9bc2-cf191ab7c14e",
      "metadata": {
        "id": "41e88248-466c-476e-9bc2-cf191ab7c14e"
      },
      "outputs": [],
      "source": [
        "# Set up Python environment\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5344404b-bb79-4225-89ad-926910470ad7",
      "metadata": {
        "id": "5344404b-bb79-4225-89ad-926910470ad7"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import argparse\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "from functools import partial\n",
        "import os\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
        "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Reproducibility\n",
        "seed = 42\n",
        "set_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac46ef3-0616-4e81-839a-f01a70f9afc9",
      "metadata": {
        "tags": [],
        "id": "fac46ef3-0616-4e81-839a-f01a70f9afc9"
      },
      "source": [
        "### Download LLaMA 2 model\n",
        "As mentioned before, LLaMA 2 models come in different flavors which are 7B, 13B, and 70B. Your choice can be influenced by your computational resources. Indeed, larger models require more resources, memory, processing power, and training time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8600826-5007-4216-bb30-cf5d52d6b89c",
      "metadata": {
        "id": "e8600826-5007-4216-bb30-cf5d52d6b89c"
      },
      "source": [
        "To download the model you have been granted access to, **make sure you are logged in to the Hugging Face model hub**. As mentioned in the requirements step, you need to use the `huggingface-cli login` command.\n",
        "\n",
        "The following function will help us to download the model and its tokenizer. It requires a bitsandbytes configuration that we will define later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a743864d-24e9-42b3-b802-aba01f540939",
      "metadata": {
        "id": "a743864d-24e9-42b3-b802-aba01f540939"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name, bnb_config):\n",
        "    n_gpus = torch.cuda.device_count()\n",
        "    max_memory = f'{40960}MB'\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
        "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
        "\n",
        "    # Needed for LLaMA tokenizer\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5788a6e-88b3-4020-9402-cd61c3f68e37",
      "metadata": {
        "id": "f5788a6e-88b3-4020-9402-cd61c3f68e37"
      },
      "source": [
        "### Download a Dataset\n",
        "\n",
        "There are many datasets that can help you fine-tune your model. You can even use your own dataset!\n",
        "\n",
        "In this tutorial, we are going to download and use the [Databricks Dolly 15k dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k), which contains 15,000 prompt/response pairs. It was crafted by over 5,000 [Databricks](https://www.databricks.com/) employees during March and April of 2023.\n",
        "\n",
        "This dataset is designed specifically for fine-tuning large language models. Released under the [CC BY-SA 3.0 license](https://creativecommons.org/licenses/by-sa/3.0/), it can be used, modified, and extended by any individual or company, even for commercial applications. So it's a perfect fit for our use case!\n",
        "\n",
        "However, like most datasets, this one has its limitations. Indeed, pay attention to the following points:\n",
        "- It consists of content collected from the public internet, which means it may contain objectionable, incorrect or biased content and typo errors, which could influence the behavior of models fine-tuned using this dataset.\n",
        "- Since the dataset has been created for Databricks by their own employees, it's worth noting that the dataset reflects the interests and semantic choices of Databricks employees, which may not be representative of the global population at large.\n",
        "- We only have access to the `train` split of the dataset, which is its largest subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd721d09-8a4d-4431-b324-78744c74a38f",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "90a8c2ed0e404066a6affc745ec0d916",
            "4a5df2fb51974f2ab751fbcb454504b6",
            "859b38f702574524ac5aefaad8b072b1",
            "9f62f587a4cd444f9c603d912dafe78d",
            ""
          ]
        },
        "id": "bd721d09-8a4d-4431-b324-78744c74a38f",
        "outputId": "53e1ef33-dec2-4367-844d-38833a67c7d3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90a8c2ed0e404066a6affc745ec0d916",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/8.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset json/databricks--databricks-dolly-15k to /workspace/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a5df2fb51974f2ab751fbcb454504b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "859b38f702574524ac5aefaad8b072b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f62f587a4cd444f9c603d912dafe78d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset json downloaded and prepared to /workspace/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
          ]
        }
      ],
      "source": [
        "# Load the databricks dataset from Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54ec2f17-51e9-4eaa-a1c6-bbc0927dd66c",
      "metadata": {
        "id": "54ec2f17-51e9-4eaa-a1c6-bbc0927dd66c"
      },
      "source": [
        "### Explore dataset\n",
        "\n",
        "Once the dataset is downloaded, we can take a look at it to understand what it contains:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fac06d5b-ba7a-4e21-89a6-4b28ed27f6a4",
      "metadata": {
        "id": "fac06d5b-ba7a-4e21-89a6-4b28ed27f6a4",
        "outputId": "9e34809a-685c-4fa8-d99d-b1674b3c29af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of prompts: 15011\n",
            "Column names are: ['instruction', 'context', 'response', 'category']\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of prompts: {len(dataset)}')\n",
        "print(f'Column names are: {dataset.column_names}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85693faa-abd8-48e1-aabe-974d5baefc63",
      "metadata": {
        "id": "85693faa-abd8-48e1-aabe-974d5baefc63"
      },
      "source": [
        "As we can see, it is composed of 4 fields named `instruction`, `context`, `response`, `category`. Let's take a look at 3 samples to better understand what we are talking about:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c2d5b9d-f562-4464-a55e-28b67790334c",
      "metadata": {
        "id": "4c2d5b9d-f562-4464-a55e-28b67790334c",
        "outputId": "e76f3aad-df20-4462-df06-e4dd7a3c54d5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instruction</th>\n",
              "      <th>context</th>\n",
              "      <th>response</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What athlete created the 'beast quake' for the...</td>\n",
              "      <td></td>\n",
              "      <td>Marshan Lynch</td>\n",
              "      <td>open_qa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Who wrote Democracy in America?</td>\n",
              "      <td></td>\n",
              "      <td>Alexis de Tocqueville wrote Democracy in America</td>\n",
              "      <td>open_qa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What links Brazil, Uruguay, Mozambique and Angola</td>\n",
              "      <td></td>\n",
              "      <td>Colonies of Portugal</td>\n",
              "      <td>open_qa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         instruction context  \\\n",
              "0  What athlete created the 'beast quake' for the...           \n",
              "1                    Who wrote Democracy in America?           \n",
              "2  What links Brazil, Uruguay, Mozambique and Angola           \n",
              "\n",
              "                                           response category  \n",
              "0                                     Marshan Lynch  open_qa  \n",
              "1  Alexis de Tocqueville wrote Democracy in America  open_qa  \n",
              "2                              Colonies of Portugal  open_qa  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Generate random indices\n",
        "nb_samples = 3\n",
        "random_indices = random.sample(range(len(dataset)), nb_samples)\n",
        "samples = []\n",
        "\n",
        "for idx in random_indices:\n",
        "    sample = dataset[idx]\n",
        "\n",
        "    sample_data = {\n",
        "        'instruction': sample['instruction'],\n",
        "        'context': sample['context'],\n",
        "        'response': sample['response'],\n",
        "        'category': sample['category']\n",
        "    }\n",
        "    samples.append(sample_data)\n",
        "\n",
        "# Create a DataFrame and display it\n",
        "df = pd.DataFrame(samples)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a593adf-9e82-4222-bd00-40128b1ce1ec",
      "metadata": {
        "id": "9a593adf-9e82-4222-bd00-40128b1ce1ec"
      },
      "source": [
        "As we can see, each sample is a dictionary that contains:\n",
        "- **An instruction**: What could be entered by the user, such as a question\n",
        "- **A context**: Help to interpret the sample\n",
        "- **A response**: Answer to the instruction\n",
        "- **A category**: Classify the sample between Open Q&A, Closed Q&A, Extract information from Wikipedia, Summarize information from Wikipedia, Brainstorming, Classification, Creative writing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00184f2a-a607-4db3-aa84-de11be305d63",
      "metadata": {
        "id": "00184f2a-a607-4db3-aa84-de11be305d63",
        "outputId": "071ff77e-8f26-4d5e-fce9-cd07429ab17c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'instruction': 'What links Brazil, Uruguay, Mozambique and Angola', 'context': '', 'response': 'Colonies of Portugal', 'category': 'open_qa'}\n"
          ]
        }
      ],
      "source": [
        "print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80902c95-62f8-42ea-8bae-3362a9a81fb6",
      "metadata": {
        "id": "80902c95-62f8-42ea-8bae-3362a9a81fb6"
      },
      "source": [
        "If you look at several samples, you will see that most do not contain any `context`. But don't worry, it doesn't matter. What we need to do now is to pre-process our data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a5f2bf1-f798-4f1b-b89f-2e2b6b3862f6",
      "metadata": {
        "id": "2a5f2bf1-f798-4f1b-b89f-2e2b6b3862f6"
      },
      "source": [
        "### Pre-processing dataset\n",
        "\n",
        "Instruction fine-tuning is a common technique used to fine-tune a base LLM for a specific downstream use-case.\n",
        "\n",
        "It will help us to format our prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2ba5cac-d7de-4c2a-82f1-45ee3d1631f8",
      "metadata": {
        "id": "a2ba5cac-d7de-4c2a-82f1-45ee3d1631f8",
        "outputId": "2ff8eba3-cbe5-45ac-820c-31834fd6858e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What links Brazil, Uruguay, Mozambique and Angola\n",
            "\n",
            "### Response:\n",
            "Colonies of Portugal\n",
            "\n",
            "### End\n"
          ]
        }
      ],
      "source": [
        "def create_prompt_formats(sample):\n",
        "    \"\"\"\n",
        "    Format various fields of the sample ('instruction', 'context', 'response')\n",
        "    Then concatenate them using two newline characters\n",
        "    :param sample: Sample dictionnary\n",
        "    \"\"\"\n",
        "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "    INSTRUCTION_KEY = \"### Instruction:\"\n",
        "    INPUT_KEY = \"### Input:\"\n",
        "    RESPONSE_KEY = \"### Response:\"\n",
        "    END_KEY = \"### End\"\n",
        "\n",
        "    blurb = f\"{INTRO_BLURB}\"\n",
        "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
        "    input_context = f\"{INPUT_KEY}\\n{sample['context']}\" if sample[\"context\"] else None\n",
        "    response = f\"{RESPONSE_KEY}\\n{sample['response']}\"\n",
        "    end = f\"{END_KEY}\"\n",
        "\n",
        "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
        "\n",
        "    formatted_prompt = \"\\n\\n\".join(parts)\n",
        "\n",
        "    sample[\"text\"] = formatted_prompt\n",
        "\n",
        "    return sample\n",
        "\n",
        "\n",
        "print(create_prompt_formats(sample)[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6481e991-48a0-4606-8e50-267d9a85f276",
      "metadata": {
        "id": "6481e991-48a0-4606-8e50-267d9a85f276"
      },
      "source": [
        "As we can see, each part is now delimited by hashtags that describe the prompt.\n",
        "\n",
        "Now, we will use our model tokenizer to process these prompts into tokenized ones. The goal is to create input sequences of uniform length (which are suitable for fine-tuning the language model because it maximizes efficiency and minimize computational overhead), that must not exceed the model's maximum token limit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d11e4c8-91b8-404a-baf2-ac0f2879ccb6",
      "metadata": {
        "id": "7d11e4c8-91b8-404a-baf2-ac0f2879ccb6"
      },
      "outputs": [],
      "source": [
        "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
        "def get_max_length(model):\n",
        "    conf = model.config\n",
        "    max_length = None\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max lenth: {max_length}\")\n",
        "            break\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max length: {max_length}\")\n",
        "    return max_length\n",
        "\n",
        "\n",
        "def preprocess_batch(batch, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Tokenizing a batch\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "\n",
        "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
        "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
        "    \"\"\"Format & tokenize it so it is ready for training\n",
        "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
        "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
        "    \"\"\"\n",
        "\n",
        "    # Add prompt to each sample\n",
        "    print(\"Preprocessing dataset...\")\n",
        "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
        "\n",
        "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
        "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
        "    dataset = dataset.map(\n",
        "        _preprocessing_function,\n",
        "        batched=True,\n",
        "        remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
        "    )\n",
        "\n",
        "    # Filter out samples that have input_ids exceeding max_length\n",
        "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
        "\n",
        "    # Shuffle dataset\n",
        "    dataset = dataset.shuffle(seed=seed)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1ed5ec3-5eab-4cdf-aafb-fe63ebc3400e",
      "metadata": {
        "id": "f1ed5ec3-5eab-4cdf-aafb-fe63ebc3400e"
      },
      "source": [
        "With these functions, our dataset will be ready for fine-tuning !"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "818ffcfe-0bba-4674-9ea5-6d7a0386a34d",
      "metadata": {
        "id": "818ffcfe-0bba-4674-9ea5-6d7a0386a34d"
      },
      "source": [
        "### Create bnb config\n",
        "\n",
        "This will allow us to load our LLM in 4 bits. This way, we can divide the used memory by 4 and import the model on smaller devices. We choose to apply bfloat16 compute data type and nested quantization for memory-saving purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d5bd1e7-f66f-4b66-9dd5-18865fd2f865",
      "metadata": {
        "id": "4d5bd1e7-f66f-4b66-9dd5-18865fd2f865"
      },
      "outputs": [],
      "source": [
        "def create_bnb_config():\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "\n",
        "    return bnb_config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3c1a41f-6dd1-4527-b1b6-c7cbea5f29b0",
      "metadata": {
        "id": "b3c1a41f-6dd1-4527-b1b6-c7cbea5f29b0"
      },
      "source": [
        "To leverage the LoRa method, we need to wrap the model as a PeftModel.\n",
        "\n",
        "To do this, we need to implement a [LoRa configuration](https://huggingface.co/docs/peft/conceptual_guides/lora):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9f5889-76b2-43c8-95da-0e48403c01d3",
      "metadata": {
        "id": "4a9f5889-76b2-43c8-95da-0e48403c01d3"
      },
      "outputs": [],
      "source": [
        "def create_peft_config(modules):\n",
        "    \"\"\"\n",
        "    Create Parameter-Efficient Fine-Tuning config for your model\n",
        "    :param modules: Names of the modules to apply Lora to\n",
        "    \"\"\"\n",
        "    config = LoraConfig(\n",
        "        r=16,  # dimension of the updated matrices\n",
        "        lora_alpha=64,  # parameter for scaling\n",
        "        target_modules=modules,\n",
        "        lora_dropout=0.1,  # dropout probability for layers\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    return config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "536fd42b-3db1-4811-8540-bfa68046d3e3",
      "metadata": {
        "id": "536fd42b-3db1-4811-8540-bfa68046d3e3"
      },
      "source": [
        "Previous function needs the target modules to update the necessary matrices. The following function will get them for our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b6cd44-e1a9-45d8-9c03-5af27717d23c",
      "metadata": {
        "id": "c9b6cd44-e1a9-45d8-9c03-5af27717d23c"
      },
      "outputs": [],
      "source": [
        "# SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
        "def find_all_linear_names(model):\n",
        "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94b6d9e2-1aad-4a61-bbe4-b112d2c04ed8",
      "metadata": {
        "id": "94b6d9e2-1aad-4a61-bbe4-b112d2c04ed8"
      },
      "source": [
        "Once everything is set up and the base model is prepared, we can use the print_trainable_parameters() helper function to see how many trainable parameters are in the model. We expect the lora_model to have fewer trainable parameters compared to the original one, since we want to perform fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb81bcf0-e2ef-4920-922a-08e02804870f",
      "metadata": {
        "id": "eb81bcf0-e2ef-4920-922a-08e02804870f"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model, use_4bit=False):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        num_params = param.numel()\n",
        "        # if using DS Zero 3 and the weights are initialized empty\n",
        "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
        "            num_params = param.ds_numel\n",
        "\n",
        "        all_param += num_params\n",
        "        if param.requires_grad:\n",
        "            trainable_params += num_params\n",
        "    if use_4bit:\n",
        "        trainable_params /= 2\n",
        "    print(\n",
        "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "210921ab-0780-4520-bb32-0321f20c39e1",
      "metadata": {
        "id": "210921ab-0780-4520-bb32-0321f20c39e1"
      },
      "source": [
        "### Training\n",
        "\n",
        "Now that everything is ready, we can pre-process our dataset and load our model using the set configurations.\n",
        "\n",
        "Then, we can run our fine-tuning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100e7441-b477-4784-827a-57726993a9ae",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "40b72ab451b1470cb2a6684e4c313a80",
            "8a53d429885b47f28cd5ee7d73b6646a",
            "0d1017c78c7840ebb0ac0185f394b6a4",
            "26da8a11911b477cb8324d2557462f46",
            "513de3bd425145a0afe35053fa054dc6",
            "f44ec41936334b7fbb1461fbec763a6b",
            "75e1ad022acb42cdbd3b16f3ca26a090",
            "92f8ad73987e4b44afa89e5dcfc1152c",
            "0376e6ec55f74b3593bc88590f495260",
            "69058fbed0e140c7ae3e5cf46014f4c3",
            "7ffca36e94934824aff576b58ed61296"
          ]
        },
        "id": "100e7441-b477-4784-827a-57726993a9ae",
        "outputId": "377a5aae-a71c-4f72-d41d-edc9d4e8c751"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40b72ab451b1470cb2a6684e4c313a80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a53d429885b47f28cd5ee7d73b6646a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d1017c78c7840ebb0ac0185f394b6a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26da8a11911b477cb8324d2557462f46",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "513de3bd425145a0afe35053fa054dc6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f44ec41936334b7fbb1461fbec763a6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75e1ad022acb42cdbd3b16f3ca26a090",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspace/.miniconda3/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:628: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92f8ad73987e4b44afa89e5dcfc1152c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0376e6ec55f74b3593bc88590f495260",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69058fbed0e140c7ae3e5cf46014f4c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ffca36e94934824aff576b58ed61296",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load model from HF with user's token and with bitsandbytes config\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "bnb_config = create_bnb_config()\n",
        "model, tokenizer = load_model(model_name, bnb_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9207b5a-cda6-434b-94e0-2439a9186ca0",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            ""
          ]
        },
        "id": "d9207b5a-cda6-434b-94e0-2439a9186ca0",
        "outputId": "6396f029-21ae-49b0-9ada-a1ec924e9c3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found max lenth: 4096\n",
            "Preprocessing dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/15011 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Preprocess dataset\n",
        "max_length = get_max_length(model)\n",
        "dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eb067e4-d8dd-4244-9b0e-8a35d4d5d1dd",
      "metadata": {
        "id": "5eb067e4-d8dd-4244-9b0e-8a35d4d5d1dd",
        "outputId": "12620a94-8f77-4bb2-8dc8-8141776cd937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all params: 3,540,389,888 || trainable params: 39,976,960 || trainable%: 1.1291682911958425\n",
            "torch.float32 302387200 0.08541070604255438\n",
            "torch.uint8 3238002688 0.9145892939574456\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 00:38, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.544200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.918400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.645800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.603600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.578300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.278700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.326800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.325600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.312500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.231300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.102100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.807400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.138600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.413800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.229300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =        0.0\n",
            "  total_flos               =   216915GF\n",
            "  train_loss               =     1.4304\n",
            "  train_runtime            = 0:00:41.69\n",
            "  train_samples_per_second =      1.439\n",
            "  train_steps_per_second   =       0.36\n",
            "{'train_runtime': 41.6903, 'train_samples_per_second': 1.439, 'train_steps_per_second': 0.36, 'total_flos': 232910960836608.0, 'train_loss': 1.430437167485555, 'epoch': 0.0}\n",
            "Saving last checkpoint of the model...\n"
          ]
        }
      ],
      "source": [
        "def train(model, tokenizer, dataset, output_dir):\n",
        "    # Apply preprocessing to the model to prepare it by\n",
        "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # Get lora module names\n",
        "    modules = find_all_linear_names(model)\n",
        "\n",
        "    # Create PEFT config for these modules and wrap the model to PEFT\n",
        "    peft_config = create_peft_config(modules)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # Print information about the percentage of trainable parameters\n",
        "    print_trainable_parameters(model)\n",
        "\n",
        "    # Training parameters\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset,\n",
        "        args=TrainingArguments(\n",
        "            per_device_train_batch_size=1,\n",
        "            gradient_accumulation_steps=4,\n",
        "            warmup_steps=2,\n",
        "            max_steps=15,\n",
        "            learning_rate=2e-4,\n",
        "            fp16=True,\n",
        "            logging_steps=1,\n",
        "            output_dir=\"outputs\",\n",
        "            optim=\"paged_adamw_8bit\",\n",
        "        ),\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "    )\n",
        "\n",
        "    model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
        "\n",
        "    ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
        "    # Verifying the datatypes before training\n",
        "\n",
        "    dtypes = {}\n",
        "    for _, p in model.named_parameters():\n",
        "        dtype = p.dtype\n",
        "        if dtype not in dtypes: dtypes[dtype] = 0\n",
        "        dtypes[dtype] += p.numel()\n",
        "    total = 0\n",
        "    for k, v in dtypes.items(): total+= v\n",
        "    for k, v in dtypes.items():\n",
        "        print(k, v, v/total)\n",
        "\n",
        "    do_train = True\n",
        "\n",
        "    # Launch training\n",
        "    print(\"Training...\")\n",
        "\n",
        "    if do_train:\n",
        "        train_result = trainer.train()\n",
        "        metrics = train_result.metrics\n",
        "        trainer.log_metrics(\"train\", metrics)\n",
        "        trainer.save_metrics(\"train\", metrics)\n",
        "        trainer.save_state()\n",
        "        print(metrics)\n",
        "\n",
        "    ###\n",
        "\n",
        "    # Saving model\n",
        "    print(\"Saving last checkpoint of the model...\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    trainer.model.save_pretrained(output_dir)\n",
        "\n",
        "    # Free memory for merging weights\n",
        "    del model\n",
        "    del trainer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "output_dir = \"results/llama2/final_checkpoint\"\n",
        "train(model, tokenizer, dataset, output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de32bc00-da6d-4ee9-b1d1-a569410aa250",
      "metadata": {
        "id": "de32bc00-da6d-4ee9-b1d1-a569410aa250"
      },
      "source": [
        "*If you prefer to have a number of epochs (entire training dataset will be passed through the model) instead of a number of training steps (forward and backward passes through the model with one batch of data), you can replace the `max_steps` argument by `num_train_epochs`.*\n",
        "\n",
        "To later load and use the model for inference, we have used the `trainer.model.save_pretrained(output_dir)` function, which saves the fine-tuned model's weights, configuration, and tokenizer files.\n",
        "\n",
        "Unfortunately, you may have noticed that the latest weights are not the best. To solve this problem, you can implement a `EarlyStoppingCallback`, from transformers, during your fine-tuning. This will enable you to regularly test your model on the validation set, if you have one, and keep only the best weights."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "995c2b15-e23f-42f3-8085-64804e459834",
      "metadata": {
        "id": "995c2b15-e23f-42f3-8085-64804e459834"
      },
      "source": [
        "### Merge weights\n",
        "\n",
        "Once we have our fine-tuned weights, we can build our fine-tuned model and save it to a new directory, with its associated tokenizer. By performing these steps, we can have a memory-efficient fine-tuned model and tokenizer ready for inference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecfe5764-7285-4c5a-aadf-8d0339c4f3ce",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "b15c5262e2f94c169abe42b3b7bfe6d1"
          ]
        },
        "id": "ecfe5764-7285-4c5a-aadf-8d0339c4f3ce",
        "outputId": "7930713b-5db2-4219-d459-a546120c2c74"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b15c5262e2f94c169abe42b3b7bfe6d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('results/llama2/final_merged_checkpoint/tokenizer_config.json',\n",
              " 'results/llama2/final_merged_checkpoint/special_tokens_map.json',\n",
              " 'results/llama2/final_merged_checkpoint/tokenizer.json')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "output_merged_dir = \"results/llama2/final_merged_checkpoint\"\n",
        "os.makedirs(output_merged_dir, exist_ok=True)\n",
        "model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
        "\n",
        "# save tokenizer for easy inference\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.save_pretrained(output_merged_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0130bf35-225f-423f-9307-bcb04542068e",
      "metadata": {
        "id": "0130bf35-225f-423f-9307-bcb04542068e"
      },
      "source": [
        "### Inference\n",
        "\n",
        "Once fine-tuned, you can test your model with an input text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8601f56-07ea-4aef-8cb5-6276eaad7883",
      "metadata": {
        "id": "c8601f56-07ea-4aef-8cb5-6276eaad7883",
        "outputId": "f5576311-bb22-4bc0-bf0d-b42b065d8612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is OVHcloud?\n",
            " Below is an instruction that describes a task.\n",
            "\n",
            "### Instruction:\n",
            "What is OVHcloud?\n",
            "\n",
            "### Response:\n",
            "OVHcloud is a French cloud computing company that provides a range of services including hosting\n"
          ]
        }
      ],
      "source": [
        "# Specify input\n",
        "text = \"What is OVHcloud?\"\n",
        "\n",
        "# Specify device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Tokenize input text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Get answer\n",
        "# (Adjust max_new_tokens variable as you wish (maximum number of tokens the model can generate to answer the input))\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"], max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# Decode output & print it\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc7b3382-d735-424e-a6bf-5b4dfe71b5be",
      "metadata": {
        "id": "dc7b3382-d735-424e-a6bf-5b4dfe71b5be"
      },
      "source": [
        "#### Push model to Hugging Face Hub (Optional)\n",
        "\n",
        "*To follow this part, make sure you logged in with a `Write` access token when you used the `huggingface-cli login` command.*\n",
        "\n",
        "If you want to share your model with others, you can push your model and your token to the Hub, in a new repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a6a2246-8f80-4b30-bfc8-072d450bdf4b",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "738e97e5140d4b56abcd7219d918ba97",
            "78819e717d6945188674018aeee8dabb",
            "7d9330f52bc64c8c906b3032b1d7e07e"
          ]
        },
        "id": "9a6a2246-8f80-4b30-bfc8-072d450bdf4b",
        "outputId": "162a952a-6189-4616-c8d5-acc7c55c3c4f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "738e97e5140d4b56abcd7219d918ba97",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78819e717d6945188674018aeee8dabb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d9330f52bc64c8c906b3032b1d7e07e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/MathieuBsqt/llama2-fine-tuned-dolly-15k/commit/2604cd28012fa6e8d4894a27ed3d1c1736f7b2f2', commit_message='Upload LlamaForCausalLM', commit_description='', oid='2604cd28012fa6e8d4894a27ed3d1c1736f7b2f2', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.push_to_hub(\"llama2-fine-tuned-dolly-15k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c03296c8-1e74-4a31-bcce-a35fbfbe6905",
      "metadata": {
        "id": "c03296c8-1e74-4a31-bcce-a35fbfbe6905",
        "outputId": "ca1e7acf-f201-4532-c245-a942a1d19273"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/MathieuBsqt/llama2-fine-tuned-dolly-15k/commit/b3892099e88abd83dd7342ae2df94e512e74a962', commit_message='Upload tokenizer', commit_description='', oid='b3892099e88abd83dd7342ae2df94e512e74a962', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.push_to_hub(\"llama2-fine-tuned-dolly-15k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c9a735c-2b5d-4543-8405-2bf300d74ec6",
      "metadata": {
        "id": "9c9a735c-2b5d-4543-8405-2bf300d74ec6"
      },
      "source": [
        "Once commited, everyone can use your fine-tuned model by using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4c2090a-3aed-4390-980e-b6dcb33805d8",
      "metadata": {
        "id": "c4c2090a-3aed-4390-980e-b6dcb33805d8"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"<username>/llama2-fine-tuned-dolly-15k\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"<username>/llama2-fine-tuned-dolly-15k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74387d33-dc65-48d2-a74c-1436d02292f0",
      "metadata": {
        "id": "74387d33-dc65-48d2-a74c-1436d02292f0"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "We hope you have enjoyed this tutorial!\n",
        "\n",
        "You are now able to fine-tune LLaMA 2 models on your own datasets !\n",
        "\n",
        "In our next tutorial, you will discover how to **Deploy your Fine-tuned LLM on OVHcloud AI Deploy for inference** !"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Conda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}