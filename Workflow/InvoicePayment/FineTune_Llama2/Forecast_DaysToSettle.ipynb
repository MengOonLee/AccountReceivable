{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/MengOonLee/AccountReceivable/blob/main/Workflow/InvoicePayment/FineTune_Llama2/Forecast_DaysToSettle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"%%bash\npip install --no-cache-dir -qU \\\n    datasets transformers bitsandbytes accelerate peft trl","metadata":{"id":"EXcpaxYZBN9F","execution":{"iopub.status.busy":"2023-10-31T08:26:43.407715Z","iopub.execute_input":"2023-10-31T08:26:43.407970Z","iopub.status.idle":"2023-10-31T08:27:14.368813Z","shell.execute_reply.started":"2023-10-31T08:26:43.407946Z","shell.execute_reply":"2023-10-31T08:27:14.367784Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":160,"referenced_widgets":["17ac4fd5de5e455699b3fdd0e9c91eb8","9142f58e83784a22bb4300fedd1789ea","d721259887f04d60a34968b01d0b124e","bffed7a7f4394dccb0590bd73c6b8446","e13f3b6dd9c14ca7bd5c24da2332d102","ff12535bd50a4542b32c83fd85df11a6","b0d59b4f667b456d94d54f26773518ab","c839728ecd744d7383b060d25645d543","08befddcd5bd4e8aac8ca798980c365c","8db6a9f40f6e41399db0e0c9ccfb435c","1a1fdaa96ee44d73b09885266775b8f1","411add3a97d948088fabf65fae939ab1","761ad4c22a01417098d4b95539489fac","39a8bd0a15674b8493bf4deb46525585","cafa8847a2934e7285af2dcdddf8a4c8","79c75cb457314576a8ba2e6f9c7b781e","9d0d936cfba84f2ea92f8fb318cfd8c3","62aaf56619484ed6a6e2945972aba530","22f8996dd1e94afda6899d1ccd5da520","28c37b8531ad4667b18eec5c6d46b131","d299f85fa6b74b3684729e4cf1b8738e","0ea367a902bc4bf4a38895248a00e5c8","faa35652100744d6a07c9942f55df0dd","7f974ca9d0d645caab050e18c637f71c","866dc5745161470d82581478798bcb05","6637624f81b24238ad515dcfbc4bd70b","9ae650856c2949d88848da09bc8d1bae","529bd9323f7b4dffb004b31da1d30926","a517bd5172bf4644b0b22cf22b5a0434","5dfbf210835c4e95952d3ab50f711ead","2108f9ebe6284720ac9f29d606130a80","b374d02735894e8989bb5a043b3019d7"]},"id":"AQYZn0P1NDp7","outputId":"3d5711a6-a5c6-4300-a963-7d1dd48d085b","execution":{"iopub.status.busy":"2023-10-31T08:27:37.384751Z","iopub.execute_input":"2023-10-31T08:27:37.385114Z","iopub.status.idle":"2023-10-31T08:27:37.728554Z","shell.execute_reply.started":"2023-10-31T08:27:37.385082Z","shell.execute_reply":"2023-10-31T08:27:37.727680Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b2c72bce44c4eba93666548084d4945"}},"metadata":{}}]},{"cell_type":"code","source":"import datasets\n\ndataset = datasets.load_dataset(\"mlabonne/guanaco-llama2-1k\", split='train')\nprint(dataset[500])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"njNGxYX7VSg_","outputId":"0b34f906-9fad-4922-cda3-2adf50314df5","execution":{"iopub.status.busy":"2023-10-31T08:29:08.664146Z","iopub.execute_input":"2023-10-31T08:29:08.664886Z","iopub.status.idle":"2023-10-31T08:29:11.565797Z","shell.execute_reply.started":"2023-10-31T08:29:08.664850Z","shell.execute_reply":"2023-10-31T08:29:11.564769Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/1.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b35e133148eb453da34ef64e2645e8f5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df4bc2a794be40c39608548a5680e3fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/967k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7894faf5f514953878bb5e9ae2ffd55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb4b4b4f7b374674a91e1dc0b9b6f799"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"429e13f5200545f38b7edfa93863e4cc"}},"metadata":{}},{"name":"stdout","text":"{'text': '<s>[INST] What is the link between race and intelligence? [/INST] There is no scientific evidence to support a link between race and intelligence. Intelligence is a complex trait that is influenced by many factors, including genetics, environment, education, and culture. Race, on the other hand, is a social construct that is based on physical characteristics such as skin color, hair texture, and facial features. While there may be genetic differences between individuals of different races, these differences do not correspond to any meaningful differences in intelligence.\\n\\nIt is important to recognize that any attempts to link race and intelligence can have harmful and divisive effects on society. The notion that certain races are inherently more or less intelligent than others has been used throughout history to justify discrimination and oppression, and it has no scientific basis. It is essential to base our understanding of intelligence on rigorous scientific research, free from bias and prejudice, and to reject any attempts to use intelligence as a means of perpetuating discrimination or inequality. </s><s>[INST] \"The role of genetics in determining measured differences in mean IQ between putative racial groups has been a focus of intense discussion and disagreement for more than 50 years. While the last several decades of research have definitively demonstrated that genetic variation can influence measures of cognitive function, the inferences drawn by some participants in the controversy regarding the implications of these findings for racial differences in cognitive ability are highly dubious. Of equal importance, there is no compelling scientific rationale for focusing on and devoting substantial effort to determining mean differences in intelligence or other cognitive functions between groups with incompletely defined and dynamic (and therefore not definitively definable) boundaries.\"\\n\\nvia: Greenspan NS. Genes, Heritability, \\'Race\\', and Intelligence: Misapprehensions and Implications. Genes (Basel). 2022 Feb 15;13(2):346. doi: 10.3390/genes13020346. PMID: 35205392; PMCID: PMC8872358. [/INST] This says pretty much what I said before. Links between race and intelligence was controversial a while back. The scientific community today however agrees that there are no links between race and intelligence. Sure, factors like diet, mental support, physical health, etc. during childhood may affect the development of cognitive functions. Studies that show a link between race and intelligence ignore this point. Such studies thus succumb to the causal fallacy, where they incorrectly conclude lower intelligence in certain parts of the world to be caused by race due to statistical correlations. In reality however, such areas face issues like poverty, malnutrition, war, etc. which lead to lower cognitive development during childhood. </s>'}\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport transformers\n\nbase_model = \"meta-llama/Llama-2-7b-chat-hf\"\n\nquantization_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=quantization_config,\n    device_map='auto'\n)\nmodel.config.use_cache = False\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\n    base_model, use_auth_token=True)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"Ja-lbrvhfh5m","outputId":"2dee9e15-609a-4722-84bc-eed9905d8035","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["ad70e1ba8c4f40e7aa9d0b3cc57b25e6","24369fb5ac5744c081849dd901f4f55b","ec979a5f90464352b2fa333601ab899f","c8d9b9ca13fa43c1ac84bdbd352f762e","25177daadd2b41a2a672e60fb02be262","477da06e758e41c5878ba2ba4c4ce814","87ad78dbbf3049fea44964233c2b76b1","112192ec62c34040a5534fd5c6b79272","0d35f8630b114dd0a3100ea2c93e4f8c","513830efcf54423085dd69b7046b9316","abc86de588534fed99d213aac2d84954"]},"execution":{"iopub.status.busy":"2023-10-31T08:29:36.888567Z","iopub.execute_input":"2023-10-31T08:29:36.889205Z","iopub.status.idle":"2023-10-31T08:31:24.985794Z","shell.execute_reply.started":"2023-10-31T08:29:36.889168Z","shell.execute_reply":"2023-10-31T08:31:24.984646Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42ddfa7433ec4094892f2689cc06b2ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87301446c0554483a57bd26c3f779ff6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"347fa0b11b6f4f778481d7d4fc19dcc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2251e37f995d40e39e64073c8b602a2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84c970217cb24cef964bb7afebb9a91d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99e63d51edec454bb30a405a1e67e8b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8684372a9b574f889762dc5ae9590634"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:655: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0ca1042b1484f44b123704303e23780"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"888aca4e69104abfa80686fdf33ef24a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a396cacb6e694baab5bc69aa6c77de91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c486ce8fa9a24802a43114fd1fc15e8c"}},"metadata":{}}]},{"cell_type":"code","source":"import transformers\nimport peft\nimport trl\n\ntraining_args = transformers.TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_32bit\",\n    logging_steps=1,\n    learning_rate=2e-4,\n    fp16=True,\n    max_steps=15,\n    warmup_steps=2\n)\n\npeft_config = peft.LoraConfig(\n    lora_alpha=64, lora_dropout=0.1, r=16,\n    bias='none', task_type='CAUSAL_LM'\n)\n\ntrainer = trl.SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field='text',\n    max_seq_length=None,\n    tokenizer=tokenizer,\n    args=training_args,\n    packing=False\n)\n\ntrainer.train()\n\npeft_model = \"peft_model\"\ntrainer.model.save_pretrained(peft_model)\n\ndel model\ndel trainer\ntorch.cuda.empty_cache()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"2xntOL6UDRph","outputId":"bd7541d2-32fe-4546-f47c-aff7f8a276a7","execution":{"iopub.status.busy":"2023-10-31T08:35:06.436148Z","iopub.execute_input":"2023-10-31T08:35:06.436847Z","iopub.status.idle":"2023-10-31T08:37:59.701615Z","shell.execute_reply.started":"2023-10-31T08:35:06.436813Z","shell.execute_reply":"2023-10-31T08:37:59.700346Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:173: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bc87454ccec4255874f3ca1c7806a9f"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231031_083553-wq1fa2e6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mengoonlee/huggingface/runs/wq1fa2e6' target=\"_blank\">scary-hex-5</a></strong> to <a href='https://wandb.ai/mengoonlee/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mengoonlee/huggingface' target=\"_blank\">https://wandb.ai/mengoonlee/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mengoonlee/huggingface/runs/wq1fa2e6' target=\"_blank\">https://wandb.ai/mengoonlee/huggingface/runs/wq1fa2e6</a>"},"metadata":{}},{"name":"stderr","text":"You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15/15 01:24, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.462300</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.452700</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.005700</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.799800</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.672000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.472400</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.781000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.522200</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.864800</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.060800</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>2.361800</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>2.320700</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.182900</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.817900</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.090500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport transformers\nimport peft\n\npretrained_model = transformers.AutoModelForCausalLM.from_pretrained(\n    base_model,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.bfloat16,\n    device_map='auto'\n)\nfinetuned_model = peft.PeftModel.from_pretrained(\n    pretrained_model, peft_model\n)\nfinetuned_model = finetuned_model.merge_and_unload()\nfinetuned_dir = \"finetuned_Llama2_forecast_DaysToSettle\"\nfinetuned_model.save_pretrained(finetuned_dir)\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\n    base_model, use_auth_token=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.save_pretrained(finetuned_dir)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T09:02:54.244179Z","iopub.execute_input":"2023-10-31T09:02:54.245121Z","iopub.status.idle":"2023-10-31T09:04:38.615666Z","shell.execute_reply.started":"2023-10-31T09:02:54.245078Z","shell.execute_reply":"2023-10-31T09:04:38.614466Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee2cbdf94ae242818d2bc61ad0331998"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:655: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"('finetuned_Llama2_forecast_DaysToSettle/tokenizer_config.json',\n 'finetuned_Llama2_forecast_DaysToSettle/special_tokens_map.json',\n 'finetuned_Llama2_forecast_DaysToSettle/tokenizer.model',\n 'finetuned_Llama2_forecast_DaysToSettle/added_tokens.json',\n 'finetuned_Llama2_forecast_DaysToSettle/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"import transformers\n\nprompt = \"What is a large language model?\"\npipe = transformers.pipeline(\n    task=\"text-generation\", \n    model=finetuned_model,\n    tokenizer=tokenizer)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"id":"POwsn9T7XUNF","execution":{"iopub.status.busy":"2023-10-31T09:09:12.831967Z","iopub.execute_input":"2023-10-31T09:09:12.832405Z","iopub.status.idle":"2023-10-31T09:10:11.690874Z","shell.execute_reply.started":"2023-10-31T09:09:12.832371Z","shell.execute_reply":"2023-10-31T09:10:11.689835Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"<s>[INST] What is a large language model? [/INST]  A large language model (LLM) is a type of artificial intelligence (AI) model that is trained on a large dataset of text to generate language outputs that are coherent and natural-sounding. The model can be used for a wide range of natural language processing (NLP) tasks, such as text generation, language translation, and text summarization.\n\nThe key advantage of LLMs is their ability to generate text that is coherent and natural-sounding, even when the input is ambiguous or unclear. This is because the model has been trained on a large dataset of text and can draw on this knowledge to generate coherent and fluent output.\n\nThere are several different types of LLMs, including:\n\n1. Recurrent neural networks (RNNs): These are the most common type of LLM and use a loop of recurrent connections to generate text one word or one character at a time.\n2. Transformers: These are a type of deep learning model that use self-attention mechanisms to generate text in parallel, rather than sequentially.\n3. Generative adversarial networks (GANs): These are a type of LLM that use two neural networks in opposition to generate text that is coherent and natural-sounding.\n4. Variational autoencoders (VAEs): These are a type of LLM that use a probabilistic approach to generate text that is coherent and natural-sounding.\n\nLLMs have many potential applications, such as:\n\n1. Language translation: LLMs can be used to translate text from one language to another, with the model generating text in the target language that is coherent and natural-sounding.\n2. Text summarization: LLMs can be used to summarize long documents or articles, generating a shorter summary that still conveys the main points of the original text.\n3. Chatbots: LLMs can be used to generate responses to user input in chatbots, with the model generating text that is coherent and natural-sounding.\n4. Content generation: LLMs can be used to generate content, such as articles, blog posts, or social media posts, that is coherent and natural-sounding.\n\nHowever, LLMs also have some limitations, such as:\n\n1. Lack of context: LLMs may not always understand the context of the input text, which can lead to inappropriate or nonsensical output.\n2. Limited domain knowledge: LLMs may not have knowledge of a specific domain or topic, which can lead to inappropriate or nonsensical output.\n3. Difficulty with ambiguity: LLMs may struggle with ambiguous or unclear input text, which can lead to inappropriate or nonsensical output.\n4. Difficulty with style: LLMs may struggle to generate text that has a consistent style, which can lead to inappropriate or nonsensical output.\n\nIn conclusion, LLMs are powerful tools for generating coherent and natural-sounding text, with a wide range of potential applications. However, they also have limitations, such as a lack of context, limited domain knowledge, difficulty with ambiguity, and difficulty with style.\n","output_type":"stream"}]}]}